---
title: "Chapter 5 Homework"
author: "Rongkui Han"
date: "5/9/2019"
output: 
  html_document: 
    keep_md: yes
---
### Easy
#### 5E1. 
Which of the linear models below are multiple linear regressions?

$$
(1) \mu_i = \alpha + x_i \\
(2) \mu_i = \beta_xx_i + \beta_zz_i \\
(3) \mu_i = \alpha + \beta(x_i - z_i)\\
(4) \mu_i = \alpha + \beta_xx_i + \beta_zz_i
$$

> (2) and (4).  

#### 5E3. 
Write down a multiple regression to evaluate the claim: Neither amount of funding nor size of laboratory is by itself a good predictor of time to PhD degree; but together these variables are both positively associated with time to degree. Write down the model definition and indicate which side of zero each slope parameter should be on.

> $\mu_i = \beta(x_i \times z_i)$, $\beta$ is positive.  

### Medium   

#### 5M2. 
Invent your own example of a masked relationship. An outcome variable should be correlated with both predictor variables, but in opposite directions. And the two predictor variables should be correlated with one another.      

> The effect of climate change on photosynthesis rate: 

> Two predictor variables: higher atmospheric CO2 content ~ higher temperature. 
> Higher atmospheric CO2 = higher Rubisco activity = elevated photosynthesis level;
> Higher temperature = higher O2 affinity of Rubisco = higher rate of photorespiration = reduced photosynthesis level.    

#### 5M3. 
It is sometimes observed that the best predictor of fire risk is the presence of firefightersâ€” States and localities with many firefighters also have more fires. Presumably firefighters do not cause fires. Nevertheless, this is not a spurious correlation. Instead fires cause firefighters. Consider the same reversal of causal inference in the context of the divorce and marriage data. How might a high divorce rate cause a higher marriage rate? Can you think of a way to evaluate this relationship, using multiple regression?    

> Divorce causes re-marriage. If all marriages are logged equally, ie 1st, 2nd, 3rd etc. marriages all contribute equally to marriage rate, then high divorce rate causes high marriage rate.   

> Since we already know that age of marriage has causal correlation with divorce rate, to evaluate the causal relationship bewtween divorce rate and marriage rate, we can to test this DAG:    
A -> M, D -> M, A -> D     
against this DAG:     
A -> M, A -> D

```{r}
library(dagitty)
dag5m3.0 <- dagitty( "dag {
A -> M
D -> M
A -> D
}")
coordinates(dag5m3.0) <- list( x=c(A=0,D=2,M=1) , y=c(A=1,D=1,M=0) )
plot( dag5m3.0 )
```

```{r}
dag5m3.1 <- dagitty( "dag {
A -> M
A -> D
}")
coordinates(dag5m3.1) <- list( x=c(A=0,D=2,M=1) , y=c(A=1,D=1,M=0) )
plot( dag5m3.1 )
```

> To do so we need to do regressions: 
1. M = $\beta_A$A  
2. M ~ $\beta_D$D   
3. M ~ $\beta_A$A + $\beta_D$D   

> If the third model makes $\beta_D$ disappear, we know there might not be causal correlation between divorce rate and marriage rate. If $\beta_D$ is still different from 0 in model 3, we might conclude that divorce rate has causal correlation with marriage rate.   

#### 5M4. 
In the divorce data, States with high numbers of Mormons (members of The Church of Jesus Christ of Latter-day Saints, LDS) have much lower divorce rates than the regression models expected. Find a list of LDS population by State and use those numbers as a predictor variable, predicting divorce rate using marriage rate, median age at marriage, and percent LDS population (possibly standardized). You may want to consider transformations of the raw percent LDS variable.     

Load and merge datasets: 
```{r}
library(rethinking)
data("WaffleDivorce")
d = WaffleDivorce
head(d)
mormons = read.csv("/Users/rongkui/Desktop/StatisticalRethinking/RethinkingV2_Rongkui.Han/Mormons.csv", header = TRUE, stringsAsFactors = FALSE)
head(mormons)
library(dplyr)
dm = merge(d, mormons, by = "Location")
head(dm)
mormons$Location[(mormons$Location %in% dm$Location)==FALSE] #nevada is not in the wattledivorce dataset. 
```

```{r}
dm$D = scale(dm$Divorce)
dm$M = scale(dm$Marriage)
dm$A = scale(dm$MedianAgeMarriage)
dm$LDS = scale(dm$Percentage.of.Mormon.Residents)
dm = dm[,c("Location","D","M","A","LDS")]
head(dm)
```

```{r}
m.5m3 = quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bM*M + bA*A + bL*LDS,
    a ~ dnorm(0,0.2),
    bM ~ dnorm(0,0.5),
    bA ~ dnorm(0,0.5),
    bL ~ dnorm(0,0.5),
    sigma ~ dexp(1)
  ), data = dm
)
precis(m.5m3)
```
 

### Hard. 
All three exercises below use the same data, `data(foxes)` (part of `rethinking`). The urban fox (*Vulpes vulpes*) is a successful exploiter of human habitat. Since urban foxes move in packs and defend territories, data on habitat quality and population density is also included. The data frame has five columns:   
(1) group: Number of the social group the individual fox belongs to   
(2) avgfood: The average amount of food available in the territory   
(3) groupsize: The number of foxes in the social group   
(4) area: Size of the territory   
(5) weight: Body weight of the individual fox   
   
#### 5H1. 
Fit two bivariate Gaussian regressions, using quap: (1) body weight as a linear function of territory size (area), and (2) body weight as a linear function of groupsize. Plot the results of these regressions, displaying the MAP regression line and the 95% interval of the mean. Is either variable important for predicting fox body weight?   

```{r}
data("foxes")
head(foxes)
foxes$W = scale(foxes$weight)
foxes$A = scale(foxes$area)
foxes$GS = scale(foxes$groupsize)
```

(1) W ~ A:    
Prior predictive simulation:    
```{r}
set.seed(2971)
N = 100 # 100 lines
a_sim = rnorm(N , 0 , 0.5)
b_sim = rnorm(N , 0 , 0.5)
plot(NULL , xlim=range(foxes$A) , ylim=c(-2,2), xlab="area" , ylab="weight")
abline(h=0 , lty=2)
abline(h=272 , lty=1 , lwd=0.5)
mtext("b ~ dnorm(0,0.5)")
xbar = mean(foxes$A)
for (i in 1:N) {
  curve(a_sim[i] + b_sim[i]*(x - xbar), from=min(foxes$A), to=max(foxes$A), add=TRUE, col=col.alpha("black",0.2))
  }
```

Calculate posterior:
```{r}
m.5h1.1 = quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- a + bA*A,
    a ~ dnorm(0, 0.5),
    bA ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = foxes)
precis(m.5h1.1)
```

Plot:
```{r}
post <- extract.samples( m.5h1.1 )
a_map <- mean(post$a)
bA_map <- mean(post$bA)
A_seq = seq(from = -2, to = 2, by = 0.01)
mu = link(m.5h1.1, data = data.frame(A = A_seq))
mu.mean = apply(mu, 2, mean)
mu.HPDI = apply(mu, 2, HPDI, prob = 0.95)
plot(W ~ A , data=foxes , col=rangi2)
lines(A_seq, mu.mean)
shade(mu.HPDI, A_seq)
curve( a_map + bA_map*(x - xbar) , add=TRUE )
```

(2) W ~ GS:   
Calculate posterior:
```{r}
m.5h1.2 = quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- a + bGS*GS,
    a ~ dnorm(0, 0.5),
    bGS ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = foxes)
precis(m.5h1.2)
```

Plot:
```{r}
post2 <- extract.samples(m.5h1.2)
a_map <- mean(post2$a)
bGS_map <- mean(post2$bGS)
GS_seq = seq(from = -2, to = 3, by = 0.01)
mu2 = link(m.5h1.2, data = data.frame(GS = GS_seq))
mu2.mean = apply(mu2, 2, mean)
mu2.HPDI = apply(mu2, 2, HPDI, prob = 0.95)
xbar = mean(foxes$GS)
plot(W ~ GS , data=foxes , col=rangi2)
lines(GS_seq, mu2.mean)
shade(mu2.HPDI, GS_seq)
```

> Group size is important.  

#### 5H2. 
Now fit a multiple linear regression with weight as the outcome and both area and groupsize as predictor variables. Plot the predictions of the model for each predictor, holding the other predictor constant at its mean. What does this model say about the importance of each variable? Why do you get different results than you got in the exercise just above?    

```{r}
m.5h2 = quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- a + bA*A + bGS*GS,
    a ~ dnorm(0, 0.5),
    bA ~ dnorm(0, 0.5),
    bGS ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = foxes)
precis(m.5h2)
```

Holding territory area as constant:   
```{r}
# prepare new counterfactual data
GS_seq = seq(from=-2 , to=3 , length.out=50)
pred_data = data.frame( GS = GS_seq , A = 0 )
# compute counterfactual mean divorce (mu)
mu_counter <- link( m.5h2 , data=pred_data )
mu_mean_counter <- apply( mu_counter , 2 , mean )
mu_PI_counter <- apply( mu_counter , 2 , PI )
# simulate counterfactual divorce outcomes
W_sim <- sim( m.5h2 , data=pred_data , n=1e4 )
W_PI <- apply( W_sim , 2 , PI )
# display predictions, hiding raw data with type="n"
plot( W ~ GS , data=foxes , type="n" )
mtext( "territory area (std) = 0" )
lines(GS_seq , mu_mean_counter )
shade( mu_PI_counter , GS_seq )
shade( W_PI , GS_seq )
```

Holding group size as constant:   
```{r}
# prepare new counterfactual data
A_seq = seq(from=-2 , to=3 , length.out=50)
pred_data2 = data.frame( GS = 0 , A = A_seq )
# compute counterfactual mean divorce (mu)
mu_counter2 <- link( m.5h2 , data=pred_data2 )
mu_mean_counter2 <- apply( mu_counter2 , 2 , mean )
mu_PI_counter2 <- apply( mu_counter2 , 2 , PI )
# simulate counterfactual divorce outcomes
W_sim2 <- sim( m.5h2 , data=pred_data2 , n=1e4 )
W_PI2 <- apply( W_sim2 , 2 , PI )
# display predictions, hiding raw data with type="n"
plot( W ~ A , data=foxes , type="n" )
mtext( "territory area (std) = 0" )
lines(A_seq , mu_mean_counter2 )
shade( mu_PI_counter2 , A_seq )
shade( W_PI2 , A_seq )
```

> Masking effect between the two variables?
